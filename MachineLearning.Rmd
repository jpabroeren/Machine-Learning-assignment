---
title: "Machine Learning of excercise"
author: "J.P.A. Broeren"
date: "17 February 2016"
output: html_document
---

***Summary***:
This is an R Markdown document for the Assignemnt of the Coursera Machine Learning course.
In this document, I will perform a machine learning on data generated by fitness trackers, on people doing barbel lifting, in a correct, and 5 incorrect ways. These data are analyzed to try and find if it is possible to predict (on basis of new data) whether or not the excersize was performed correctly.

So first, load in the training and test set, after loading in the required packages:
```{r}
library(caret)
library(randomForest)
train_data <- read.csv ("pml-training.csv")
test_data <- read.csv ("pml-testing.csv")
```
First up, we need to clean up the data. One of the first things to do, is remove any column that is not necessary. The first column, 'X', is just an index, and as such has no aditional information. So I remove this. Also, a first look at the data suggests that there are a lot of parameters, that seem to be very low in variance. To ease the calculations, determine whether or not this intuition is correct. If so, remove these columns. 
Lastly, there are columns wit a lot of NA values. These probably also won't lead to additional insights. Therefore, remove the columns with more than 75% of NA values. 
```{r}
lowVariances <- nearZeroVar(train_data, saveMetrics=FALSE)
#Remove these low-or-zero-Variances:
train_data <- subset(train_data, select = -lowVariances)
test_data <- subset(test_data, select = -lowVariances)
#for some reason they are not of the same types, so change these in the testing data:
test_data[] <- mapply(FUN = as,test_data,sapply(train_data,class),SIMPLIFY = FALSE)

#for creating the trainingset, remove all columns with NAs > 75%:
#Catch is, I must do that from the test_data first, otherwise I have to go over it row by row, or the wrong ones could be eliminated...
test_data <- test_data[colMeans(is.na(train_data)) <= 0.75]
train_data <- train_data[colMeans(is.na(train_data)) <= 0.75]

train_data <- subset(train_data, select = -c(X))
test_data <- subset(test_data, select = -c(X, problem_id))
#Lastly, apparently I have to adjust the 'levels' of the test_data timestamp to correspond with those from the train_data. So do that now:
levels (test_data$cvtd_timestamp) <- levels(train_data$cvtd_timestamp)
```
At this point, we're down to 58 variables (instead of 160). This should ease the calculations a bit.
Next step is to create the random forest. But before I do that, split the data in a training set, and a set for cross-validation. I do 75/25%:
```{r}
set.seed (1)    #why would I take another one?

#Create the crossvalidation test-set:
forTraining <- createDataPartition(y=train_data$classe, p=0.75, list=FALSE)
crossValidate <- train_data[-forTraining,]
train_data <- train_data[forTraining,]
#Before I actually perform the training, split into a test and 
RFModel <- randomForest(classe ~ ., data = train_data)
RFCrossValidate <- predict(RFModel, crossValidate)

#And also the eventual set for the answering:
RFPrediction <- predict(RFModel, test_data)
```

Now we can have a look at the confusionmatrix, to test whether the model holds up:
```{r}
confusionMatrix(RFCrossValidate, crossValidate$classe)
```
This looks rather promising.
But let's calculate what the prediction errors are on the crossvalidation set (out-of-sample Error). For this, compare the predicted classe with the actual classe on the crossvalidation set.
```{r}
outOfSampleAccuracy <- sum (RFCrossValidate == crossValidate$classe)/length(RFCrossValidate)
paste0 ("Out of sample accuracy is: ", round(outOfSampleAccuracy * 100, digits = 3), " %")
#So the sample error is:
paste0 ("Out of sample error estimate is: ", round((1 - outOfSampleAccuracy) * 100, digits = 3), " %")
```
The eventual testing-set yields the following results (these values will be the answers on the final quiz):
```{r}
RFPrediction
```
